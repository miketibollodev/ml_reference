{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "8db24a8c09619303"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Scaling\n",
    "**Scaling**: the process of changing the *magnitude* (scale) of numerical features so they are comparable across dimensions.\n",
    "\n",
    "For *distance-based* or *gradient-based* models (i.e., linear regression, logistic regression, neural networks, k-nearest networks), not scaling features can:\n",
    "- Distort distance calculations\n",
    "- Slow or destabilize optimization\n",
    "- Cause uneven regularization penalties\n",
    "- Reduce model performance\n"
   ],
   "id": "1607fa1c87550b06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Importance of Feature Scaling\n",
    "- Linear and logistic regression models compute predictions as weighted linear combinations of features. When features have different scales (say $[0, 100]$ and $[0, 10]$), the larger magnitude features can dominate (as $100w_1>>10w_2$). While the model could adjust weights to compensate, differences in scale affect the numerical conditioning of the optimization problem.\n",
    "- Gradient-based optimization is sensitive to feature scale. Uneven feature magnitudes tend to elongate loss curves leading to inefficient gradient updates and slower converge. Scaling makes the curvature of the loss function more isotropic.\n",
    "- Distance-based models rely directly on distance metrics such as Euclidean distance. If one feature has a larger scale than others, it disproportionately influences distance calculations, reducing the contribution of smaller-scale features.\n",
    "- Regularization penalize coefficient magnitudes rather than feature magnitudes. Without scaling, features with larger ranges require smaller coefficients, while smaller-scale features require larger coefficients. Thus, regularization will unevenly penalize the coefficients.\n",
    "\n",
    "Generally speaking, many learning algorithms benefit from standardization of data. Tree-based models are unaffected by data that is not scaled, as it does not compute products, distances, or gradients. Instead, they make splits on threshold conditions. Scaling preserves the ordering of values, so if a feature scaled from $[0, 500]$ to $[0, 1]$, a split at $F_1 < 20$ remains identical at $F_1 < 0.04$."
   ],
   "id": "59943d196ae10846"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Transformers\n",
    "**Preprocessing Transformers**: transformers are anything that learns something from data (fit) and then modifies data (transform). We have three types of preprocessing transformers to scale data:\n",
    "- **Scalers**: operate feature-wise and adjust the mean, variance, range, and spread of data. They preserve the order, linear relationships, and overall shape of distributions. Most scalers perform *linear transformations*.\n",
    "- **Transformers**: operate feature-wise and apply *non-linear transformations*, meaning they modify the shape of the distribution. Used when data is highly skewed or variance depends on magnitude.\n",
    "- **Normalizers**: operate sample-wise and rescale each data point to have a unit norm."
   ],
   "id": "d79def26b17ce4c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Encoding\n",
    "**Encoding**: the process of converting categorical features into numerical representations."
   ],
   "id": "de7b737377388221"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ordinal Encoding\n",
    "Ordinal encoding is for categories with a meaningful order. `OrdinalEncoder` preserves order. It is critical only ordinal data uses an ordinal encoding strategy, as higher values imply higher ranking in the order.\n",
    "| Size   | Encoded |\n",
    "| ------ | ------- |\n",
    "| Small  | 0       |\n",
    "| Medium | 1       |\n",
    "| Large  | 2       |"
   ],
   "id": "c2663eed1eb69f0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Nominal Encoding\n",
    "Nominal encoding is for categories that do not have a meaningful order. `OneHotEncoder` creates a binary column for each category. This effectively produces a \"new feature\" for each category.\n",
    "\n",
    "| Color | Red | Blue | Green |\n",
    "| ----- | --- | ---- | ----- |\n",
    "| Red   | 1   | 0    | 0     |\n",
    "| Blue  | 0   | 1    | 0     |\n",
    "| Green | 0   | 0    | 1     |"
   ],
   "id": "b1ffa9edfb67ca96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
