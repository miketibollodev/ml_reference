{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ensemble Models\n",
    "**Ensemble Models**: technique that aggregates two or more learners, with the underlying assumption that the collective results from many learners is better than an individual learner.\n",
    "\n",
    "Research suggests that, in general, the greater diversity among combined models, the more accurate the resulting model. Furthermore, ensembles of diverse under-regularized models outperform individual regularized models."
   ],
   "id": "3f24218603ec8742"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Parallel Methods\n",
    "Parallel methods train each base learner independently of the others."
   ],
   "id": "dc1e48b995917914"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Bagging\n",
    "**Bagging (Bootstrap Aggregating)**: learners are trained on a different dataset derived from *bootstrap resampling*. This effectively randomly chooses points from the entire dataset (with replacement) so each learner has a different subset of the training set. The predictions of each learner are then aggregated to give a final prediction (i.e., majority in classification, mean in regression).\n",
    "\n",
    "Generally, the underlying learners tend to be overfit such that noise is well-captured, and then aggregated with other results to dampen the effect."
   ],
   "id": "87ab82184eb724db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Random Forests\n",
    "**Random Forests**: popular version of a bagging model, where the base learner is always a decision tree. Each tree will use a bootstrapped sample, but will also select a random subset of the features for every split node decision. This additional randomness decorrelates the prediction errors of the individual trees."
   ],
   "id": "bcec5581c6d0f28f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stacking\n",
    "**Stacking**: several base learners are trained using the same dataset but with different training algorithms. Each base learner makes predictions on a testing set, then, these predictions are compiled and used as features to train a *metamodel*."
   ],
   "id": "e3502e27ff993999"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sequential Methods\n",
    "Sequential methods train a new base learner to minimize the errors made by the previous model."
   ],
   "id": "17fc3ed5c5b86609"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Adaptive Boosting\n",
    "**Adaptive Boosting**: trains an underfit model and makes predictions. For the misclassified samples or samples with the highest errors, the next learner is trained with a higher weight on these samples."
   ],
   "id": "d9e68aa4a681cc28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Gradient Boosting\n",
    "**Gradient Boosting**: trains an underfit model (typically a decision tree) and makes predictions. For the next iteration, the learner will predict the residuals instead of the target."
   ],
   "id": "471114c718417f4e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
