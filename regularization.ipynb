{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Regularization\n",
    "**Regularization**: a method for controlling model complexity by discouraging extreme parameter values during training (i.e., prevent overfitting). It *reduces variance* by *incorporating bias*, through additional terms on the objective function to penalize large or unstable values. Regularization helps control overfitting with the following issues:\n",
    "- When models become highly expressive compared to the amount of data, there is a high amount of variance. This restricts how extreme parameters can become by limiting the effective capacity of the model without changing its functional form.\n",
    "- When features are highly correlated (multicollinearity) or the number of features approaches or exceeds the number of samples, coefficients can explode to large magnitudes and cancel each other out. This makes the coefficients numerically stable."
   ],
   "id": "50d162c60f85d515"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are numerous techniques for regularization. Aside from the linear model regularization techniques, there is:\n",
    "- **Data Augmentation**: modifying existing training data to create artificial data samples from pre-existing samples (i.e., rotating or grayscaling an image).\n",
    "- **Early Stopping**: limiting the number of iterations during training."
   ],
   "id": "f375131a52c6de81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Linear Model Regularization\n",
    "*Note*: for the techniques below, we generally want to tune the hyperparameter `alpha` or `l1_ratio`."
   ],
   "id": "5e058c92172bb43e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### L1 Regularization\n",
    "**Lasso Regression**: technique that penalizes high-value, correlated coefficients. It introduces a penalty term into the loss function, which is the absolute value of the sum of the coefficients. Its strength is controlled by $\\lambda$ (`alpha`)."
   ],
   "id": "904cf48e42ad9012"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$ loss=\\frac{1}{n}\\Sigma(y_i-\\hat{y_i}) + \\lambda\\Sigma|w_i| $$",
   "id": "3bf15c53cdf9d130"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T20:46:24.038135Z",
     "start_time": "2026-02-25T20:46:23.287071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "print(\"Coefficients:\", lasso.coef_)"
   ],
   "id": "83a7fa7b816ce713",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.06362439921332558\n",
      "Coefficients: [60.50305581 98.52475354 64.3929265  56.96061238 35.52928502]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### L2 Regularization\n",
    "**Ridge Regression**: technique that also penalizes high-value coefficients, except it adds a penalty term that is the sum of squared coefficients. While lasso regression can enact feature selection (remove features outright), ridge regression can drive coefficients towards zero but never to zero. Its strength is controlled by $\\lambda$ (`alpha`)."
   ],
   "id": "579080c62f830c50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$ loss=\\frac{1}{n}\\Sigma(y_i-\\hat{y_i}) + \\lambda\\Sigma{w_i}^2 $$",
   "id": "849483cee42ee736"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T20:46:40.788466Z",
     "start_time": "2026-02-25T20:46:40.749132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Coefficients:\", ridge.coef_)"
   ],
   "id": "d4f5c8de5c2a17e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.114050771972588\n",
      "Coefficients: [59.87954432 97.15091098 63.24364738 56.31999433 35.34591136]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Elastic Net Regularization\n",
    "**Elastic Net Regularization**: combines L1 and L2 regularization by adding both L1 and L2 penalty terms into the loss function. Thus, it addresses multicollinearity while enabling feature selection. $\\alpha$ (`l1_ratio`) is introduced to control the ratio of L1 to L2."
   ],
   "id": "39c27989bafc1e01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$ loss=\\frac{1}{n}\\Sigma(y_i-\\hat{y_i}) + \\lambda((1-\\alpha)\\Sigma|w_i|+\\alpha\\Sigma{w_i}^2) $$\n",
   "id": "e36fa390547f2bf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T20:46:49.431544Z",
     "start_time": "2026-02-25T20:46:49.397391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Coefficients:\", model.coef_)"
   ],
   "id": "59d0259201f1ef88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 7785.886176938016\n",
      "Coefficients: [16.84528938 31.77080959  4.05901996 40.18486737 57.25856154 45.81463318\n",
      " 58.97979422 -0.          3.82816854 41.1096051 ]\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
