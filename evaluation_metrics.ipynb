{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation Metrics",
   "id": "30884d6ed5c2653f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classification Evaluation",
   "id": "bf0c5b2021c44401"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Metrics",
   "id": "629ea26ecef4ebd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Confusion Matrix**: table that summarizes the performance of a classification algorithm based on true positives, true negatives, false positives, and false negatives classified.",
   "id": "f70768c6022511d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Accuracy**: the fundamental metric used to evaluate performance. It measures the proportion of correctly predicted instances among all instances.\n",
    "\n",
    "$$ accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "Accuracy can be misleading when deadling with imbalanced classes. For example, a class with 70/30 imbalanced sets would have a 70% accuracy score if using a dummy classifier with a most-frequent strategy."
   ],
   "id": "f75dea93bd86d07c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Precision**: assesses the quality of positive predictions made by a classification model. It is the proportion of true positives to all positive predictions. This would be valuable in contexts like medical diagnoses or spam detection; we want to ensure low numbers of false positive predictions.\n",
    "\n",
    "$$ precision = \\frac{TP}{TP + FP} $$"
   ],
   "id": "318a969a6b59b3f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Recall**: assesses the ability to correctly identify all positive instances within a dataset. It is the proportion of true positives among all positives. Again, this could be critical in medical diagnoses or security, where we want to ensure we are not missing true cases.\n",
    "\n",
    "$$ recall = \\frac{TP}{TP + FN} $$"
   ],
   "id": "6e8a588cbfa9dee4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Specificity**: the true negative rate.\n",
    "\n",
    "$$ specificity = \\frac{TN}{TN + FP} $$"
   ],
   "id": "aff37576a2fa4840"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**F1-Score**: metric that combines precision and recall into one metric. It is especially useful when there is class imbalance.\n",
    "\n",
    "$$ f1 = 2(\\frac{precision * recall}{precision + recall}) $$"
   ],
   "id": "39d41a150d0142ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Curves",
   "id": "3cd772dc2a5631fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Receiver Operating Characteristic (ROC) Curve**: representation of the model's ability to distinguish between positive and negative classes by plotting the specificity (x) against the recall (y). The ideal curve would be in the top left corner, where the false positive rate is 0.0 and the true positive rate is 1.0.",
   "id": "2b0ddcad573e3473"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Precision-Recall Curve**: representation of how well the model predicts the positive class. It plots the recall (x) against the precision (y). The ideal curve would be in the top right corner.",
   "id": "2b84534f4fe4954f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Area Under Curve (AUC)**: provides a single salar value that summarizes the model using the ROC or PRC.",
   "id": "5d4af0852cb0d4d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Regression Evaluation",
   "id": "7784e7839e03c6c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Metrics",
   "id": "b01248947adfae17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mean Absolute Error (MAE)**: the average absolute difference between actual and predicted values. It is easy to interpret as it uses the same unit as the target variable, but it treats all errors equally regardless of direction.\n",
    "\n",
    "$$ MAE = \\frac{1}{n}\\Sigma|y_i-\\hat{y_i}| $$"
   ],
   "id": "efc7415960332b66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mean Squared Error (MSE)**: the average squared difference between actual and predicted values. It penalizes larger errors more strongly, making it sensitive to outliers. It is also more difficult to interpret.\n",
    "\n",
    "$$ MSE = \\frac{1}{n}\\Sigma(y_i-\\hat{y_i})^2 $$"
   ],
   "id": "ad75b886ca3a7fb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Root Mean Squared Error (RMSE)**: takes the root of the MSE, which maintains a strong penalty for larger errors while also improve interpretability by converting back to the original unit of the target variable.\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{1}{n}\\Sigma(y_i-\\hat{y_i})^2} $$"
   ],
   "id": "eaa09c316e748a26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**R-Squared**: represents the proportion of variance in the target variable that is explained by the regression model. In other words, the total variation measures how much the data varies overall. The unexplained variation is the variation that our model failed to capture. Thus, if we take one less the proportion of unexplained variance over total variance, we are left with the percentage of variance our model captures. We can think of the value as how much better the model is than guessing the mean.\n",
    "\n",
    "$$ R^2=1 - \\frac{Unexplained\\;variation}{Total\\;variation} = 1 - \\frac{RSS}{SST} $$\n",
    "\n",
    "where:\n",
    "- $ RSS = \\Sigma(y_i-\\hat{y_i})^2 $\n",
    "- $ SST = \\Sigma(y_i-\\bar{y_i})^2 $\n"
   ],
   "id": "ce43f2669a35d1b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mean Absolute Percentage Error**: the prediction error as a percentage of the actual values. While intuitive, it becomes unreliable when actual values are close to zero.\n",
    "\n",
    "$$ MAPE = \\frac{100}{n}\\Sigma|\\frac{y_i-\\hat{y_i}}{y_i}| $$"
   ],
   "id": "9f49f446ff7ca98b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
