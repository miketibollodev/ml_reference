{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\n",
    "Feature engineering, or preprocessing, is the process of selecting, creating, or modifying features. This can involve handling missing values, encoding categories, scaling numbers, and more. The common types of feature engineering are:\n",
    "- Feature creation\n",
    "- Feature transformation\n",
    "- Feature extraction\n",
    "- Feature selection\n",
    "- Feature scaling\n",
    "\n",
    "Cleaning the data typically occurs before feature engineering (i.e., removing records with missing data, filling in blank data, etc.). Commonly, this involves using `SimpleImputer` or `IterativeImputer` as part of a pipeline."
   ],
   "id": "8db24a8c09619303"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Scaling\n",
    "**Scaling**: the process of changing the *magnitude* (scale) of numerical features so they are comparable across dimensions.\n",
    "\n",
    "For *distance-based* or *gradient-based* models (i.e., linear regression, logistic regression, neural networks, k-nearest networks), not scaling features can:\n",
    "- Distort distance calculations\n",
    "- Slow or destabilize optimization\n",
    "- Cause uneven regularization penalties\n",
    "- Reduce model performance\n"
   ],
   "id": "1607fa1c87550b06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Importance of Feature Scaling\n",
    "- Linear and logistic regression models compute predictions as weighted linear combinations of features. When features have different scales (say $[0, 100]$ and $[0, 10]$), the larger magnitude features can dominate (as $100w_1>>10w_2$). While the model could adjust weights to compensate, differences in scale affect the numerical conditioning of the optimization problem.\n",
    "- Gradient-based optimization is sensitive to feature scale. Uneven feature magnitudes tend to elongate loss curves leading to inefficient gradient updates and slower converge. Scaling makes the curvature of the loss function more isotropic.\n",
    "- Distance-based models rely directly on distance metrics such as Euclidean distance. If one feature has a larger scale than others, it disproportionately influences distance calculations, reducing the contribution of smaller-scale features.\n",
    "- Regularization penalize coefficient magnitudes rather than feature magnitudes. Without scaling, features with larger ranges require smaller coefficients, while smaller-scale features require larger coefficients. Thus, regularization will unevenly penalize the coefficients.\n",
    "\n",
    "Generally speaking, many learning algorithms benefit from standardization of data. Tree-based models are unaffected by data that is not scaled, as it does not compute products, distances, or gradients. Instead, they make splits on threshold conditions. Scaling preserves the ordering of values, so if a feature scaled from $[0, 500]$ to $[0, 1]$, a split at $F_1 < 20$ remains identical at $F_1 < 0.04$."
   ],
   "id": "59943d196ae10846"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Transformers\n",
    "**Preprocessing Transformers**: transformers are anything that learns something from data (fit) and then modifies data (transform). We have three types of preprocessing transformers to scale data:\n",
    "- **Scalers**: operate feature-wise and adjust the mean, variance, range, and spread of data. They preserve the order, linear relationships, and overall shape of distributions. Most scalers perform *linear transformations*.\n",
    "- **Transformers**: operate feature-wise and apply *non-linear transformations*, meaning they modify the shape of the distribution. Used when data is highly skewed or variance depends on magnitude.\n",
    "- **Normalizers**: operate sample-wise and rescale each data point to have a unit norm."
   ],
   "id": "d79def26b17ce4c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Transformation\n",
    "**Encoding**: the process of converting categorical features into numerical representations."
   ],
   "id": "de7b737377388221"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ordinal Encoding\n",
    "Ordinal encoding is for categories with a meaningful order. `OrdinalEncoder` preserves order. It is critical only ordinal data uses an ordinal encoding strategy, as higher values imply higher ranking in the order.\n",
    "| Size   | Encoded |\n",
    "| ------ | ------- |\n",
    "| Small  | 0       |\n",
    "| Medium | 1       |\n",
    "| Large  | 2       |"
   ],
   "id": "c2663eed1eb69f0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Nominal Encoding\n",
    "Nominal encoding is for categories that do not have a meaningful order. `OneHotEncoder` creates a binary column for each category. This effectively produces a \"new feature\" for each category.\n",
    "\n",
    "| Color | Red | Blue | Green |\n",
    "| ----- | --- | ---- | ----- |\n",
    "| Red   | 1   | 0    | 0     |\n",
    "| Blue  | 0   | 1    | 0     |\n",
    "| Green | 0   | 0    | 1     |"
   ],
   "id": "b1ffa9edfb67ca96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Binning\n",
    "Binning transforms continuous values into categorical features. For example, a bin could be created to transform age into a set of $k$ bins. `KBinsDiscretizer` creates these bins."
   ],
   "id": "442a14eea423152e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Selection\n",
    "**Selection**: choosing relevant features to use. Sometimes features can be filtered based on statistical measures (i.e., correlation), chosen based on model performance, etc."
   ],
   "id": "3eced2095d575132"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Extraction\n",
    "**Extraction**: the creation of features from some data."
   ],
   "id": "920b0adff538b64c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Creation\n",
    "**Creation**: the process of creating new features from existing features. Typically, this requires domain expertise. This is often called feature engineering."
   ],
   "id": "98e6ab14377f87f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Polynomial Features\n",
    "There are cases where the raw features cannot be captured by a model. The polynomial of a feature or interaction of independent features can be generated. When we have domain expertise, we can explicitly create a new feature, for example, the product of two features $x_1x_2$.\n",
    "\n",
    "`PolynomialFeatures` generates the polynomial and interaction features of all features in the pipeline, for a specified `degree`. For example, if we had features $x_1$ and $x_2$, `PolynomialFeatures(degree=2)` generates $[1, x_1, x_2, x_1^2, x_2^2, x_1x_2]$.\n",
    "\n",
    "When the number of features is large or the degree is highly, dimensionality can explode. Thus, using `PolynomialFeatures` is best used with lower degrees and low dimensionality, and is better alongside a ridge/lasso to shrink useless terms."
   ],
   "id": "9eb49363c7282fb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Spline Transformer\n",
    "Splines are piecewise polynomial functions, such that polynomials are generated within intervals and stitched together at the boundary points (called knots).\n",
    "\n",
    "`SplineTransformer` allows the creation of features for a given `n_knots` and `degree`. It also provides strategies for where knots occur with `knots`."
   ],
   "id": "1adb43af7c9730d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
